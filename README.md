免责声明：
本文章涉及到的应用仅供学习交流使用，不得用于任何商业用途，数据来源于互联网公开内容，没有获取任何私有和有权限的信息（个人信息等）。由此引发的任何法律纠纷与本人无关！禁止将本文技术或者本文所关联的Github项目源码用于任何目的。
抖音团队刑事法律培训——技术中立，爬虫无罪？ 
一、创作背景
- 最近自己想尝试用gpt做下续写小说、提炼小说大纲等AIGC任务，想看看最后的效果，但是这些实验都需要大量的小说数据投喂，想要找到可以下载小说的网站或者app很不容易，而且下载的小说资源会出现badcase：最开始一些章节没有问题，但是后面的章节乱码等现象。于是探索了如何从各大官方网站上爬取相关章节（官网上的小说肯定没有错别字、乱码、缺少段落的情况），调研了node爬虫的姿势（不用python写，是因为大家都是前端工程师，对js会更加熟悉，以及发现puppeteer比python爬虫框架更加好用），写了一个通用的命令行小说爬虫项目。
- 从爬虫两种方式入手，让不太了解爬虫的同学，对爬虫的理解程度进阶；爬虫老司机也可以开阔思路，补充细节。也介绍反爬虫和反反爬虫的相关策略，让我们在开发过程中，对每个接口、模块、页面是否有爬虫风险了然于胸。

二、爬虫的方式
1. 接口请求式
暂时无法在飞书文档外展示此内容
爬取接口的原理始终都是一样的，那就是：
1. 设置好爬虫请求的相关参数（正常的浏览器User-Agent、需要携带的cookie等）
2. 通过浏览器devtools或者抓包工具筛选找到想要爬取的接口
3. 破解接口的安全加密手段
4. 构造下个接口请求的地址，找出规律
5. 开始爬虫
6. 将爬取的数据存在本地或者数据库中

2. 浏览器模拟请求式
暂时无法在飞书文档外展示此内容
当然我们也可以把思路扩大，放在整个浏览器展示页面中，直接获取页面展示的内容，而不是通过接口拿到数据了。
相比请求接口的爬虫方式，通过浏览器模拟有以下优点
1. 无需关注接口加密的方式，往往破解接口加密是爬虫工作中最困难的一步
2. 无需关注各种请求头的构造
3. 接口的反爬虫机制对浏览器访问基本失效，不用耗费精力在反爬虫机制上（比如接口请求qps、请求时间间隔等）
当然也会有一些缺点
1. 会请求很多不必要的资源，比如图片、js、css文件等等
2. 爬取速度较慢，因为模拟浏览器一个一个打开页面本就比请求接口慢
接下来对两种爬虫方式，使用真实的例子来进行解释

三、今日头条爬虫（接口爬虫方式）
请求接口的爬虫我们以爬取今日头条feed推送流以及各个大V的全部发表内容为案例
[图片]
[图片]

流程图
暂时无法在飞书文档外展示此内容

选择爬取接口、js逆向破解加密方法
feed流接口
头条的旧版feed流接口之前为https://www.toutiao.com/api/pc/feed，由于被爬虫的情况猖獗，现在已经改版了接口，改为了https://www.toutiao.com/api/pc/list/feed，之前的一套加密方案已经失效。但是经过筛选观察，发现h5版头条的仍然使用类似旧版的加密方案，有很大爬虫风险。
h5头条feed流接口https://m.toutiao.com/list/
[图片]
[图片]
观察发现：请求存在参数as、cp两个参数，as开头均为A1，cp结尾均为E1。于是可以猜想这可能是两个拼接的字符串，于是在页面源代码中搜索这两个字符串，发现的确是存在类似的代码的。
并且下一个请求的max_time、max_behot_time、i三个参数都是从前一个接口拿到的。
[图片]
这便是加密函数的具体实现，我们可以猜想下，这里W()函数可能是一种hash摘要或者加密算法。可以尝试md5、SHA系列函数尝试，最后发现这里就是md5摘要。于是我们得到了接口加密的方式，如下所示：
暂时无法在飞书文档外展示此内容

用户页面接口
用户页数据接口：https://profile.zjurl.cn/api/feed_backflow/profile_share/v1/
[图片]
[图片]
观察发现仅仅offset偏移量存在着变化，并无加密参数。但是我们要获取到每个大V的uid
下一个请求的offset参数同样是从前一个接口拿到的。

携带配置项，开始爬取
给请求构造header和cookie（应对接口反爬虫策略），发起请求。
暂时无法在飞书文档外展示此内容

构造下个接口，循环爬取
请求过程中发现，即使携带了header请求头以及cookie，仍然会被接口层的反扒策略发现，会一直返回相同的数据，也就是fake假数据。
[图片]
这是头条的蜜罐反爬虫手段，可以参考下面的文档。
头条PC个人主页反爬方案 
[图片]
纵使携带了大量真实请求的请求头和cookie，仍然能被接口反爬虫给识别出来，那该怎么办呢？
于是我们的“大杀器”——无头浏览器就出场了。
可以在puppeteer中发起请求，现在我们本身就是在浏览器中发起的请求，自然各种浏览器参数都携带了，反爬虫手段就很难识别出来了。下面是用puppeteer爬取的数据
[图片]
拿到足够多的feed流数据后，可以按照点赞数排序uid，为后续爬取大V账号做准备。
[图片]
接下来就可以选择前几个uid来爬取他们的个人首页了。
[图片]
至此，基于爬取接口方式的爬虫就介绍完毕了。拿到了这些账号uid、以及对应的作品数据，完全可以将其存在数据库中，用这些数据可以搭建自己的新闻网站（对应的正文、图片链接都能拿到）。
下面是头条爬取的示例
暂时无法在飞书文档外展示此内容
下面是头条做的一些反爬手段的文档，有兴趣的可以看看
前端反爬虫方案讨论(数值、时间等场景) 
头条PC个人主页反爬方案 

四、小说爬虫（模拟浏览器方式）
接下来到我们的“重头戏”了，来爬取各大文学网站的小说。

前置工作
为了让小说爬虫更具有通用性，需要给其添加通用配置项。等到爬取对应网站时，直接读取当前网站的配置即可。而写入配置最好是用json格式，但是传统json格式不能写注释，而为了后续阅读代码以及迭代是需要一定的注释的，于是配置项使用了hjson来定义。
[图片]
主要作用就是在json文件中编写注释，地址：https://github.com/hjson/hjson
命令行工具则是用到了readline-sync这个包，用来获取命令行的输入，并在代码中使用输入。
[图片]
地址：https://bnpm.bytedance.net/package/readline-sync

流程图
暂时无法在飞书文档外展示此内容

命令行输入配置
输入要爬取的小说地址、开始章节、结束章节、本地存储文件夹、是否合并章节、书源配置等信息，这些信息会在爬虫代码中获取到并使用。
[图片]

读取书源配置
  一本小说的书籍信息包括了
- 书名
- 书籍图片
- 作者
- 书籍介绍
- 章节内容
要获取这些信息需要观察页面的dom构造，使用dom选择器拿到我对应的dom节点，再将节点文本信息去除掉多余的符号（换行符、空格等）。
在起点中文网书籍首页拿到以上信息，将这些配置写json里
[图片]
[图片]
同理，在小说章节页面获取章节信息的dom，在json里写入配置函数
[图片]
[图片]

循环章节/存储小说
在爬取完当前章节内容后，要构造出下一章的内容。这里针对于不同网站有两种方式去完成
- 针对于下一章按钮是a标签的，获取到a标签的href，直接跳转这个页面即可。代表网站：起点、纵横、晋江、笔趣阁、noveltoon、wattpad等
[图片]
暂时无法在飞书文档外展示此内容
- 针对于下一章按钮不是a标签，而是通过click事件跳转的，可以模拟浏览器的点击事件进行跳转。代表网站：番茄小说网
[图片]
暂时无法在飞书文档外展示此内容
下面是一个小说爬虫流程示例
暂时无法在飞书文档外展示此内容

接入字体混淆如何爬取
番茄小说网接入了字体混淆，返回的章节内容很多都是特殊字符。我们再直接拿取dom结构就没用了，可以用两种方式解决
- 硬碰硬，解密字体混淆方法。分析字体文件破译字体加密方案。
- ocr识别，puppeteer截图后，ocr识别文本
[图片]
在写番茄小说网的爬虫用到的是ocr识别方案。可以使用本地的ocr模块或者直接调用各大云的ocr接口，识别准确率都接近100%（因为小说纯文字识别，没啥难度）。
下面是爬取番茄的示例。
暂时无法在飞书文档外展示此内容

五、反爬虫和反反爬虫策略
没有100%防范爬虫的方案，反爬虫的主要目的是增加爬虫的成本，尽可能促使其放弃爬取。
下面介绍下本文写作中遇到的一些反爬虫手段以及如何去解决。

真实请求检测
- 反爬策略：头条的feed流接口会判断请求是否真实。所以我们应当使爬虫请求尽可能地模拟用户真实访问。
- 反反爬：可以采取下面的方法
  - 携带大量请求头，并随机变化。可以本地写一个request header的数组，每次请求随机带上一些，或者使用第三方fake header库也可以
  - 携带页面的cookie，模拟登录态
  - 用无头浏览器爬取，例如js的puppeteer，python的selenium

请求间隔检测
- 反爬策略：一般接口都会将时间间隔固定的请求判定为爬虫。
- 反反爬：我们可以每次发送请求后，sleep随机等待时间

js加密
- 反爬策略：头条feed请求会携带as、cp两个加密参数。
- 反反爬：具体计算过程在页面源码中存在，当然源码经过了js混淆。于是可以debug找到加密部分的代码，并破解它。这也是js逆向的过程。

控制台无限debugger
- 反爬策略：晋江的页面打开devtools会自动debugger卡住。
- 反反爬：解决方法
  - 我们可以关掉devtools的开启debugger功能，这样就可以看页面的源码了
[图片]
[图片]
  - Never pause here：在debugger位置的行号处，右键Never pause here，永远不在debugger处运行
  - 条件断点，设置条件断点为false也可以跳过debugger；
[图片]
Never pause here
  在 debugger 位置，点击行号，右键 Never pause here，永远不在此处断下即可：

IP检测
- 反爬策略：接口层会将短时间大量同一ip请求判定为爬虫。
- 反反爬：我们可以构造ip池，每次请求随机使用一个。

图形验证码
- 反爬策略：一段时间后，页面可能会出现验证码识别，要完成识别后才能进行下一步操作。
- 反反爬：使用ocr识别验证码，再进行后续操作。当然使用云厂商或本地搭建都可以。

字体混淆
- 反爬策略：番茄小说内容就接入了字体混淆
- 反反爬：使用ocr识别或者破解字体加密手段

下面也列举了知乎的一些回答，有兴趣可以看看。
暂时无法在飞书文档外展示此内容
暂时无法在飞书文档外展示此内容

六、理想反爬系统架构
一个理想的反爬架构要从多个方面入手
  - 产品建设
  - 风险感知
  - 爬虫识别
  - 风险处理
[图片]

七、爬虫相关法律规定
关于爬虫的法律规定主要集中在以下几类法规中：
- 法律：刑法（1997年修订）
  - 《刑法》扰乱公共秩序罪，第 285 条，违反国家规定，侵入国家事务、国防建设、尖端科学技术领域的计算机信息系统的，处三年以下有期徒刑或者拘役。违反国家规定，侵入前款规定以外的计算机信息系统或者采用其他技术手段，获取该计算机信息系统中存储、处理或者传输的数据，或者对该计算机信息系统实施非法控制，情节严重的，处三年以下有期徒刑或者拘役，并处或者单处罚金；情节特别严重的，处三年以上七年以下有期徒刑，并处罚金。
- 法律：一般法律
  - 《中华人民共和国个人信息保护法》
  - 《中华人民共和国网络安全法》
- 行政法规
  - 2019年《数据安全管理办法（征求意见稿）》第十六条 网络运营者采取自动化手段访问收集网站数据，不得妨碍网站正常运行；此类行为严重影响网站运行，如自动化访问收集流量超过网站日均流量三分之一，网站要求停止自动化访问收集时，应当停止。

八、novel-crawler总结 
- 一个node命令行爬虫项目，只需输入要爬取小说的网页地址，即可爬取整本小说
- 目前支持小说网站（包括了国内、国际化所有的竞品）
  - 国内
    - 起点
    - 晋江
    - 七猫
    - 纵横
    - 番茄
    - 笔趣阁
  - 国外
    - noveltoon
    - wattpad
- 如果你有感兴趣的文学网站，可以仿照bookSource下的json配置添加网站的爬取方式（相信对大家来说是小case），或者提个issue、私聊@我@裴宇轩 也可以，我有时间会将该网站的爬虫配置加进去。
- 项目中的toutiao.js是用来爬取头条feed流和大V主页的，从feed流接口拿到大V的uid，再去爬取大V个人主页的全部信息
- Gitlab地址：https://code.byted.org/novel-fe/novel-crawler
- 最后再来一次免责说明：爬虫写的好，牢饭吃到饱，不要用此爬虫脚本来进行盈利或破坏行为！
免责说明
本文章涉及到的应用仅供学习交流使用，不得用于任何商业用途，数据来源于互联网公开内容，没有获取任何私有和有权限的信息（个人信息等）。由此引发的任何法律纠纷与本人无关！禁止将本文技术或者本文所关联的Gitlab项目源码用于任何目的。

